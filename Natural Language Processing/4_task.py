# -*- coding: utf-8 -*-
"""UZD4_Gelzinyte.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aAn7GV973EO0U1SP-qHHt531JP618VwC

#### <font color="red"> Uždavinių atlikimui naudokite tekstą esantį dokumente `tekstas.txt`. </font>

#### 1. Atspausdinkite antro sakinio kalbos vienetus, Pos žymas, smulkiagretes POS žymas ir jų aprašymą (angl. token text, the POS tag, the fine-grained TAG tag, and the description of the fine-grained tag). Uždavinį atlikite su spaCy biblioteka
"""

import spacy
nlp = spacy.load("en_core_web_sm")

with open('tekstas.txt', 'r') as f:
    text = f.read()

#print(text)

doc = nlp(text)
doc_sents = [sent for sent in doc.sents]
doc2 = doc_sents[1]

#kalbos vienetai
for token in doc2:
  print(token.text, end = " | ")
print('\n')
for token in doc2:
    print(f'{token.text:{15}}{token.pos_:{10}} {token.tag_:{10}} {spacy.explain(token.tag_)}')

"""#### 2. Pateikite dokumento POS žymų dažnumo sąrašą





"""

POS_counts = doc.count_by(spacy.attrs.POS)

for k,v in sorted(POS_counts.items()):
  print(f'{k}. {doc.vocab[k].text:{6}}: {v}')

"""#### 3. Atspausdinkite visus tekste esančius būdvardžius"""

!pip install textacy

import textacy

pattern = [{"POS": "ADJ"}]

talk_doc = textacy.make_spacy_doc(text, lang = "en_core_web_sm")

adj_words = textacy.extract.token_matches(talk_doc, patterns = pattern)

#spausdinam visus budvardzius
for adj in adj_words:
  print(adj.text)

"""#### 4. Atspausdinkite visus tekste esančius prielinksnius"""

pattern = [{"POS": "ADP"}]

talk_doc = textacy.make_spacy_doc(text, lang = "en_core_web_sm")

adp_words = textacy.extract.token_matches(talk_doc, patterns = pattern)

#spausdinam visus budvardzius
for adp in adp_words:
  print(adp.text)

"""#### 5. Koks procentas tokenų yra veiksmažodžiai?"""

verb = []
for token in doc:
    if token.pos_ == 'VERB':
        verb.append(token.text)
print(round((len(verb)/len(doc)*100),2),'%')

"""#### 6. Atspausdinkite 2-ro sakinio tokenus ir POS žymas. Uždavinį atlikite su NLTK biblioteka

#### <font color= blue> https://www.nltk.org/book/ch05.html </font>
"""

import nltk
from nltk.tokenize import word_tokenize
nltk.download('averaged_perceptron_tagger')
nltk.download("punkt")
sentences= nltk.sent_tokenize(text)
words = sentences[1] # isrenkam 2 sakini
sent = nltk.word_tokenize(words)

'\n'
nltk.pos_tag(sent)

"""#### 7. Iš turimo teksto (t.y. viso nuskaityto teksto) pašalinkite visus veiksmažodžius ir naujos eilutės simbolį (t.y. "\n")"""

import re

space = '\n'
temp = re.sub(space, '', text)

tokens = nltk.word_tokenize(text)

output = [w for w in tokens if not w in verb]
print(' '.join(output))

"""#### <font color="red"> Uždavinių atlikimui naudokite tekstą esantį dokumente `tekstas2.txt`. </font>

#### 8. Atspausdinkite pirmas tris įvardytas esybes, esančias tekste.¶
"""

with open('tekstas2.txt', 'r') as f:
    text2 = f.read()

import spacy
nlp = spacy.load("en_core_web_sm")

doc = nlp(text2)

i = 0

for ent in doc.ents:
    if i < 3:
        print(ent.text + " - " + ent.label_ + " - " + str(spacy.explain(ent.label_)))
        i += 1
    else:
        break

"""#### 9. Vizualizuokite ketvirto sakinio įvardytas esybes. Pakeiskite elementų fono spalvą bei pritaikykite gradiento efektą (spalvas parinkdami savo nuožiūra).¶"""

from spacy import displacy

doc = nlp(text2)

doc_sents = [sent for sent in doc.sents]
doc2 = doc_sents[3]

colors = {
    "ORG": "linear-gradient(90deg, #aa9cfc, #fc9ce7)",
    "PERSON": "linear-gradient(90deg, #a0fca0, #9cfca1)",
    "CARDINAL": "linear-gradient(90deg, #fcaaaa, #fca099)",
    "DATE": "linear-gradient(90deg, #9cfced, #9cd0fc)"

}

# Configure options with the custom color scheme
options = {"ents": ["ORG", "PERSON", "CARDINAL", "DATE"], "colors": colors}

displacy.render(doc2, style ='ent', jupyter = True, options=options )

"""#### <font color="red"> Uždavinių atlikimui naudokite tekstą esantį dokumente `tekstas3.pdf`. </font>

#### 10. Pateikite dokumento POS žymų dažnumo sąrašą
"""

!pip install PyPDF2

import spacy
nlp = spacy.load("en_core_web_sm")
import PyPDF2
file = open('tekstas3.pdf', 'rb')
text3 = PyPDF2.PdfReader(file)
pageObj = text3.pages[0]
pageObj2 = text3.pages[1]
pageObj3 = text3.pages[2]
text = pageObj.extract_text()
text = pageObj2.extract_text()
text = pageObj3.extract_text()
file.close()

doc = nlp(text)

pos_counts = doc.count_by(spacy.attrs.POS)
for k,v in sorted(pos_counts.items()):
    print(f'{doc.vocab[k].text:{6}}: {v}')