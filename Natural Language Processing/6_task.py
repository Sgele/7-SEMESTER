# -*- coding: utf-8 -*-
"""UZD6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P_jaMDMpuea5Lavb8fofHO88mazrn9OT

#### 1. Nustatykite vektorinius ryšius tarp Jūsų sugalvotų dviejų sinonimų. Žodžių vektorizavimui naudokite Word2vec. Kosinusų atstumui tarp dviejų vektorių paskaičiuoti naudokite du būdus: SpaCy bibliotekos funkciją "similarity" ir Scipy bibliotekos funkciją "cosine"
"""

!python -m spacy download en_core_web_md
!pip install scipy

import spacy
nlp = spacy.load("en_core_web_md")


print(nlp(u"boring").similarity(nlp(u"dull")))

#cosine
from scipy.spatial.distance import cosine
from gensim.models import Word2Vec

w1 = nlp.vocab["boring"].vector
w2 = nlp.vocab["dull"].vector

cosine_similarity = 1 - cosine(w1, w2)
print(cosine_similarity)

"""#### 2. Nustatykite vektorinius ryšius tarp Jūsų sugalvotų dviejų antonimų. Teksto vektorizavimui naudokite TF-IDF. Dokumentų rinkinį, susidedanti iš kelių vieno sakinio tekstų sukurkite patys"
"""

from numpy.lib.function_base import vectorize
from sklearn.feature_extraction.text import TfidfVectorizer

doc = [' a boring and exciting discussion',
       'He told us a very exciting adventure story']

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(doc)

w1_idx = vectorizer.vocabulary_["boring"]
w2_idx = vectorizer.vocabulary_["exciting"]

w1_vector = X[:, w1_idx].todense().A1
w2_vector = X[:, w2_idx].todense().A1

cosine_similarity = 1 - cosine(w1_vector, w2_vector)
print(cosine_similarity)

"""#### 3. Nustatykite vektorinius ryšius tarp 2-am uždaviniui sugalvotų dviejų antonimų. Teksto vektorizavimui naudokite TF-IDF. Naudokite vieną iš bibliotekų pateiktų pavyzdinių duomenų rinkinių (pvz. Brown corpus)"""

import nltk
nltk.download("brown")
from nltk.corpus import brown

doc1 = ['a boring  discussion',
        'He told us a very exciting adventure story']
#Brown Corpus
doc1 = [' '.join(brown.words(categories=category)) for category in brown.categories()]

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(doc1)

w1_idx = vectorizer.vocabulary_["boring"]
w2_idx = vectorizer.vocabulary_["exciting"]

w1_vector = X[:, w1_idx].todense().A1
w2_vector = X[:, w2_idx].todense().A1

cosine_similarity = 1 - cosine(w1_vector, w2_vector)
print(cosine_similarity)

"""#### 4. Nustatykite vektorinius ryšius tarp Jūsų sugalvoto hipernimo ir jo dviejų hiponimų. Teksto vektorizavimui naudokite žodžių maišą. Rezultatą pateikite lentelės pavidalu."""

from sklearn.metrics import jaccard_score
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

documents = ['I like flowers',
             'roses are red',
             'peonies are pink']

documents = [' '.join(brown.words(categories=category)) for category
in brown.categories()]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(documents)

word1_idx = vectorizer.vocabulary_["flowers"]
word2_idx = vectorizer.vocabulary_["roses"]
word3_idx = vectorizer.vocabulary_["peonies"]

word1_vector = X[:, word1_idx].todense().A1
word2_vector = X[:, word2_idx].todense().A1
word3_vector = X[:, word3_idx].todense().A1

from IPython.display import Markdown, display

display(Markdown(f'<table><tr><th></th><th>{"flower"}</th><th>{"roses"}</th><th>{"peonies"}</th></tr>\
<tr><td>{"flower"}</td><td>{1 - cosine(word1_vector, word1_vector)}</td><td>{1 - cosine(word1_vector, word2_vector)}</td><td>{1 - cosine(word1_vector, word3_vector)}</td></tr>\
<tr><td>{"roses"}</td><td>{1 - cosine(word2_vector, word1_vector)}</td><td>{1 - cosine(word2_vector, word2_vector)}</td><td>{1 - cosine(word2_vector, word3_vector)}</td></tr>\
<tr><td>{"peonies"}</td><td>{1 - cosine(word3_vector, word1_vector)}</td><td>{1 - cosine(word3_vector, word2_vector)}</td><td>{1 - cosine(word3_vector, word3_vector)}</td></tr>'))

"""#### 5. Nustatykite vektorinius ryšius tarp Jūsų sugalvoto holonimo ir jo dviejų meronimu. Rezultatą pateikite lentelės pavidalu."""

from sklearn.metrics import jaccard_score
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

documents = ['I saw a dead body',
             'I broke my arm',
             'My friend an ankle']

documents = [' '.join(brown.words(categories=category)) for category
in brown.categories()]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(documents)

word1_idx = vectorizer.vocabulary_["body"]
word2_idx = vectorizer.vocabulary_["arm"]
word3_idx = vectorizer.vocabulary_["ankle"]

word1_vector = X[:, word1_idx].todense().A1
word2_vector = X[:, word2_idx].todense().A1
word3_vector = X[:, word3_idx].todense().A1

from IPython.display import Markdown, display

display(Markdown(f'<table><tr><th></th><th>{"body"}</th><th>{"arm"}</th><th>{"ankle"}</th></tr>\
<tr><td>{"body"}</td><td>{1 - cosine(word1_vector, word1_vector)}</td><td>{1 - cosine(word1_vector, word2_vector)}</td><td>{1 - cosine(word1_vector, word3_vector)}</td></tr>\
<tr><td>{"arm"}</td><td>{1 - cosine(word2_vector, word1_vector)}</td><td>{1 - cosine(word2_vector, word2_vector)}</td><td>{1 - cosine(word2_vector, word3_vector)}</td></tr>\
<tr><td>{"ankle"}</td><td>{1 - cosine(word3_vector, word1_vector)}</td><td>{1 - cosine(word3_vector, word2_vector)}</td><td>{1 - cosine(word3_vector, word3_vector)}</td></tr>'))

"""#### 6. Raskite žodyne 5 artimiausius žodžius atitinkančius žodį „university“"""

!pip install gensim

import gensim.downloader as api
model = api.load("word2vec-google-news-300")

# Find words most similar to "university"
similar_words = model.most_similar("university", topn=10)

# Print the most similar words and their similarity scores
for word, score in similar_words:
    print(f"{word}: {score:.4f}")